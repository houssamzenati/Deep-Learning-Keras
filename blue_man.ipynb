{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "#         self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "#         self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "#         self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "        # Brute force\n",
    "        scores = []\n",
    "        index_dict = {}\n",
    "        for index, key in enumerate(self.word2vec.keys()):\n",
    "            scores += [self.score(w, key)]\n",
    "            index_dict[index] = key\n",
    "        indexes = np.flip(np.argsort(scores))\n",
    "        return [index_dict[indexes[i]] for i in range(K)]\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        e1, e2 = self.word2vec[w1], self.word2vec[w2]\n",
    "        return np.dot(e1, e2)/(np.linalg.norm(e1)*np.linalg.norm(e2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 0.7775108541288563\n",
      "germany berlin 0.7420295235998394\n",
      "['cat', 'cats', 'kitty', 'kitten', 'feline']\n",
      "['dog', 'dogs', 'puppy', 'Dog', 'doggie']\n",
      "['dogs', 'dog', 'pooches', 'Dogs', 'doggies']\n",
      "['paris', 'france', 'Paris', 'london', 'berlin']\n",
      "['germany', 'austria', 'europe', 'german', 'berlin']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'))\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipdb\n",
    "\n",
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf={}, keepdims=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        N = len(sentences)\n",
    "        for sent in sentences:\n",
    "            list_embeds = []\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    list_embeds += [self.w2v.word2vec[word]]\n",
    "                except KeyError:\n",
    "                    sent.remove(word)\n",
    "\n",
    "            embeds = np.array(list_embeds)\n",
    "            \n",
    "            if idf != {}:\n",
    "                # idf-weighted mean of word vectors\n",
    "                coeffs = np.array([max(1, np.log10(N / (idf[word]))) for word in sent])\n",
    "\n",
    "                try:\n",
    "                    average = np.average(embeds, axis=0, weights=coeffs)\n",
    "                except ValueError:\n",
    "                    break\n",
    "                sentemb += [average]\n",
    "            else:\n",
    "                emb = np.mean(embeds, axis=0)\n",
    "                # mean of word vectors\n",
    "                if emb.shape == ():\n",
    "                    emb = np.zeros(300)\n",
    "                sentemb += [emb]\n",
    "\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf={}, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        print(\"Nearest neighbors of \\\"%s\\\":\" % \" \".join(s))\n",
    "        keys = self.encode(sentences, idf)\n",
    "        query = self.encode([s], idf)\n",
    "        # Brute force\n",
    "        scores = []\n",
    "        index_dict = {}\n",
    "        for index, key in enumerate(keys):\n",
    "            scores += [self.score(query, key, idf=idf, encoded=True)]\n",
    "            index_dict[index] = sentences[index]\n",
    "        indexes = np.flip(np.argsort(scores))\n",
    "        neighbors = [index_dict[indexes[i]] for i in range(K)]\n",
    "        for elem_list in neighbors:\n",
    "            print(\" \".join(elem_list) + '\\n')\n",
    "        return neighbors\n",
    "\n",
    "    def score(self, s1, s2, idf={}, encoded=False):\n",
    "        if encoded == False:\n",
    "            s1_, s2_ = s1, s2\n",
    "            s1 = self.encode([s1], idf)\n",
    "            s2 = self.encode([s2], idf)\n",
    "            s2 = np.squeeze(s2, axis=0)\n",
    "            print(\"Compute score of \\\"%s\\\" and\" % \" \".join(s1_), \"\\%s\\\" \" % \" \".join(s2_))\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        score = float(np.dot(s1, s2)/(np.linalg.norm(s1, 2)*np.linalg.norm(s2, 2)))\n",
    "        if encoded == False: print(score)\n",
    "        return score\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            for w in set(sent):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "Nearest neighbors of \"1 woman in a black jacket is drinking out of a bottle while others are smiling\":\n",
      "1 woman in a black jacket is drinking out of a bottle while others are smiling\n",
      "\n",
      "a black man wearing a black shirt with yellow leaves on it , is sitting and drinking a cup of coffee\n",
      "\n",
      "a black man in a striped shirt stands eating out of a cup , holding a yellow water bottle\n",
      "\n",
      "a blue-eyed woman with and glasses looks up at the camera while holding a hamburger with a bite out of it , as she sits on a boat with four other young people who are drinking and eating\n",
      "\n",
      "a blond woman in a leather jacket is sitting outside with a man in a blue and red jacket , and there is a dog between them\n",
      "\n",
      "Compute score of \"1 man standing and several people sitting down waiting on a subway train\" and \\10 women dressed in long black dresses holding a booklet up sheet music in front of them singing in a you can see the back of 3 older gentlemen heads who appear to be the audience\" \n",
      "0.7797889678205021\n",
      "Nearest neighbors of \"1 woman in a black jacket is drinking out of a bottle while others are smiling\":\n",
      "1 woman in a black jacket is drinking out of a bottle while others are smiling\n",
      "\n",
      "2 guys facing away from camera , 1 girl smiling at camera with blue shirt , 1 guy with a beverage with a jacket on\n",
      "\n",
      "2 black guys in army uniforms , one is talking on a cellphone the other is in dark sunglasses , both holding on to a rail , to the left behind them is a sliding glass door with an orange logo and to the right is a palm tree\n",
      "\n",
      "4 kids playing , one in a red dress one in a pink shirt , one in a white shirt and a little boy with his arm through a green toy\n",
      "\n",
      "3 man on a boot one with dark and gray hair and gray shirt sitting inside the building , one going has very little hair green and white checked shirt blue jeans and a wristwatch on his left hand , sitting behind a rail , the other bald in all black but a white logo on his shirt leaning on the rail\n",
      "\n",
      "Compute score of \"1 man standing and several people sitting down waiting on a subway train\" and \\10 women dressed in long black dresses holding a booklet up sheet music in front of them singing in a you can see the back of 3 older gentlemen heads who appear to be the audience\" \n",
      "0.7797889678205021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7797889678205021"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'))\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = []\n",
    "nmax = 10000\n",
    "with io.open(os.path.join(PATH_TO_DATA, 'sentences.txt'), encoding='utf-8') as f:\n",
    "    next(f)\n",
    "    for i, line in enumerate(f):\n",
    "        if line.endswith('. \\n'):\n",
    "            try:\n",
    "                line, _ = line.split(' .', 1)\n",
    "            except ValueError:\n",
    "                print(line)\n",
    "            line = str(line)\n",
    "        words = line.split(' ')\n",
    "        sentences += [words]\n",
    "        if i == (nmax - 1):\n",
    "            break\n",
    "\n",
    "s2v = BoV(w2v)\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = s2v.build_idf(sentences)\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_vectors(fname, nmax=50000):\n",
    "    word2vec = {}\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "            if i == (nmax - 1):\n",
    "                break\n",
    "    print('Loaded %s pretrained word vectors' % (len(word2vec)))\n",
    "    return word2vec\n",
    "\n",
    "w2v_en = load_vectors(os.path.join(PATH_TO_DATA, 'wiki.en.vec'))\n",
    "w2v_fr = load_vectors(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "list_en, list_fr = list(w2v_en.keys()), list(w2v_fr.keys())\n",
    "common_elements = [element for element in list_en if element in list_fr]\n",
    "X = np.array([w2v_fr[key] for key in common_elements])\n",
    "Y = np.array([w2v_en[key] for key in common_elements])\n",
    "X = X.T\n",
    "Y = Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "import scipy.linalg \n",
    "U, s, Vh = scipy.linalg.svd(Y @ X.T)\n",
    "W = U @ Vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"ordinateur\":\n",
      "0.6838 - computers\n",
      "0.6785 - computer\n",
      "0.6444 - mainframe\n",
      "0.6360 - workstation\n",
      "0.6221 - programmable\n",
      "Nearest neighbors of \"table\":\n",
      "0.6442 - table\n",
      "0.5380 - tables\n",
      "0.4071 - billiard\n",
      "0.3884 - menus\n",
      "0.3865 - hash\n",
      "Nearest neighbors of \"cheval\":\n",
      "0.6120 - horse\n",
      "0.5969 - horses\n",
      "0.5676 - cheval\n",
      "0.5195 - horseman\n",
      "0.5071 - dressage\n",
      "Nearest neighbors of \"tomato\":\n",
      "0.6906 - tomates\n",
      "0.6880 - tomate\n",
      "0.6434 - haricots\n",
      "0.6255 - oignon\n",
      "0.6046 - patate\n",
      "Nearest neighbors of \"car\":\n",
      "0.7470 - voiture\n",
      "0.7055 - voitures\n",
      "0.6378 - automobile\n",
      "0.6320 - porsche\n",
      "0.6193 - automobiles\n",
      "Nearest neighbors of \"philosophy\":\n",
      "0.7947 - philosophie\n",
      "0.7556 - philosophy\n",
      "0.7215 - philosophies\n",
      "0.6678 - philosophique\n",
      "0.6665 - métaphysique\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def score(emb_1, emb_2):\n",
    "    return (emb_1 / np.linalg.norm(emb_1)).dot(emb_2 / np.linalg.norm(emb_2))\n",
    "\n",
    "def get_nn(word, src_w2v, tgt_w2v, rot, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    word_emb = src_w2v[word]\n",
    "    word_emb_to_tgt = rot @ word_emb\n",
    "    scores = []\n",
    "    index_dict = {}\n",
    "    for index, key in enumerate(tgt_w2v.keys()):\n",
    "        scores += [score(word_emb_to_tgt, tgt_w2v[key])]\n",
    "        index_dict[index] = key\n",
    "    scores = np.array(scores)\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], index_dict[idx]))\n",
    "\n",
    "for word in  ['ordinateur', 'table', 'cheval']:\n",
    "    get_nn(word, w2v_fr, w2v_en, rot = W)\n",
    "for word in  ['tomato', 'car', 'philosophy']:\n",
    "    get_nn(word, w2v_en, w2v_fr, rot = W.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "import ipdb \n",
    "# TYPE CODE HERE\n",
    "def load_sentences(fname, nmax=10000, label=True):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            words = line.split(' ')\n",
    "            if label:\n",
    "                labels += [int(words.pop(0))]\n",
    "            sentences += [words]\n",
    "            if i == (nmax - 1):\n",
    "                break\n",
    "\n",
    "    return np.array(sentences), np.array(labels)\n",
    "    \n",
    "x_train, y_train = load_sentences(os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.train'))\n",
    "x_dev, y_dev = load_sentences(os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.dev'))\n",
    "x_test, _ = load_sentences(os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.test'), label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "s2v = BoV(w2v)\n",
    "# TYPE CODE HERE\n",
    "x_train_emb = s2v.encode(x_train)\n",
    "x_dev_emb = s2v.encode(x_dev)\n",
    "x_test_emb = s2v.encode(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score with L2 penalty: 0.4726\n",
      "Dev score with L2 penalty: 0.4209\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "C = 0.002\n",
    "x_train_emb = scaler.fit_transform(x_train_emb)\n",
    "x_dev_emb = scaler.transform(x_dev_emb)\n",
    "clf_l2 = LogisticRegression(C=C, penalty='l2', tol=0.01, solver='saga')\n",
    "clf_l2.fit(x_train_emb, y_train)\n",
    "\n",
    "print(\"Training score with L2 penalty: %.4f\" % clf_l2.score(x_train_emb, y_train))\n",
    "print(\"Dev score with L2 penalty: %.4f\" % clf_l2.score(x_dev_emb, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "predictions = clf_l2.predict(x_test_emb)\n",
    "np.savetxt('logreg_bov_y_test_sst.txt', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "# PATH_TO_DATA = \"../../data/\"\n",
    "\n",
    "# TYPE CODE HERE\n",
    "train_name = os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.train')\n",
    "dev_name = os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.dev')\n",
    "test_name = os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.test')\n",
    "train_data = io.open(train_name, encoding='utf-8')\n",
    "dev_data = io.open(dev_name, encoding='utf-8')\n",
    "test_data = io.open(test_name, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "VOCAB_SIZE = len(train_data.read())\n",
    "\n",
    "def one_hot_encode(fname, label=True):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            words = line.split(' ')\n",
    "            if label:\n",
    "                labels += [int(words.pop(0))]\n",
    "            line = \" \".join(words)\n",
    "            encoded = one_hot(line, VOCAB_SIZE)\n",
    "            sentences += [encoded]\n",
    "\n",
    "    return np.array(sentences), to_categorical(np.array(labels), num_classes=5)\n",
    "\n",
    "x_train, y_train = one_hot_encode(train_name)\n",
    "x_dev, y_dev = one_hot_encode(dev_name)\n",
    "x_test, _ = one_hot_encode(test_name, label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LEN = max([len(x) for x in x_train])\n",
    "\n",
    "def pad_sequence(input_sequences):\n",
    "    return np.array(pad_sequences(input_sequences, maxlen=MAX_SEQUENCE_LEN, padding='pre'))\n",
    "\n",
    "x_train = pad_sequence(x_train)\n",
    "x_dev = pad_sequence(x_dev)\n",
    "x_test = pad_sequence(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = VOCAB_SIZE  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 32)          28887040  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 28,912,197\n",
      "Trainable params: 28,912,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8543 samples, validate on 1100 samples\n",
      "Epoch 1/6\n",
      "8543/8543 [==============================] - 107s - loss: 1.5725 - acc: 0.2718 - val_loss: 1.5680 - val_acc: 0.3036\n",
      "Epoch 2/6\n",
      "8543/8543 [==============================] - 102s - loss: 1.5131 - acc: 0.3384 - val_loss: 1.4572 - val_acc: 0.3691\n",
      "Epoch 3/6\n",
      "8543/8543 [==============================] - 110s - loss: 1.2589 - acc: 0.4686 - val_loss: 1.4003 - val_acc: 0.3736\n",
      "Epoch 4/6\n",
      "8543/8543 [==============================] - 104s - loss: 1.0084 - acc: 0.5829 - val_loss: 1.4371 - val_acc: 0.3773\n",
      "Epoch 5/6\n",
      "8543/8543 [==============================] - 105s - loss: 0.8035 - acc: 0.6952 - val_loss: 1.6947 - val_acc: 0.3573\n",
      "Epoch 6/6\n",
      "4608/8543 [===============>..............] - ETA: 47s - loss: 0.6259 - acc: 0.7728"
     ]
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "score_train = model.evaluate(x_train, y_train)\n",
    "score_dev = model.evaluate(x_dev, y_dev)\n",
    "print(\"\\n Training score: %.4f\" % score_train[1])\n",
    "print(\"\\n Dev score: %.4f\" % score_dev[1])\n",
    "predictions = model.predict(x_test)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "np.savetxt('logreg_lstm_y_test_sst.txt', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, kernel_regularizer=<keras.reg..., dropout=0.3, recurrent_dropout=0.3)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 10)          9027200   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               38400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 9,066,245\n",
      "Trainable params: 9,066,245\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8543 samples, validate on 1100 samples\n",
      "Epoch 1/20\n",
      "8543/8543 [==============================] - 132s - loss: 1.5770 - acc: 0.2702 - val_loss: 1.5730 - val_acc: 0.2536\n",
      "Epoch 2/20\n",
      "8543/8543 [==============================] - 113s - loss: 1.5519 - acc: 0.2978 - val_loss: 1.5069 - val_acc: 0.3555\n",
      "Epoch 3/20\n",
      "8543/8543 [==============================] - 107s - loss: 1.3774 - acc: 0.4070 - val_loss: 1.3817 - val_acc: 0.4064\n",
      "Epoch 4/20\n",
      "8543/8543 [==============================] - 111s - loss: 1.1721 - acc: 0.5181 - val_loss: 1.4431 - val_acc: 0.3755\n",
      "Epoch 5/20\n",
      "8543/8543 [==============================] - 114s - loss: 1.0107 - acc: 0.5967 - val_loss: 1.5179 - val_acc: 0.3618\n",
      "Epoch 6/20\n",
      "8543/8543 [==============================] - 101s - loss: 0.8769 - acc: 0.6740 - val_loss: 1.6241 - val_acc: 0.3618\n",
      "8543/8543 [==============================] - 14s    \n",
      "1088/1100 [============================>.] - ETA: 0s\n",
      " Training score: 0.5205\n",
      "\n",
      " Dev score: 0.4064\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.layers import Bidirectional, Conv1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10))\n",
    "model.add(Bidirectional(LSTM(nhid, dropout_W=0.3, dropout_U=0.3, kernel_regularizer=regularizers.l2(0.001))))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())\n",
    "\n",
    "bs = 32\n",
    "n_epochs = 20\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_dev, y_dev), callbacks=callbacks)\n",
    "\n",
    "model.load_weights('best_model.h5') # revert to the best model\n",
    "score_train = model.evaluate(x_train, y_train)\n",
    "score_dev = model.evaluate(x_dev, y_dev)\n",
    "print(\"\\n Training score: %.4f\" % score_train[1])\n",
    "print(\"\\n Dev score: %.4f\" % score_dev[1])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "np.savetxt('bilstm_y_test_sst.txt', predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
